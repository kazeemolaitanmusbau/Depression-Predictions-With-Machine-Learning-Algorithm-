{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34044af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b29c425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data into datafram\n",
    "path =\"C:/Users/pc/Desktop/1/my project/depression prediction/\"\n",
    "df = pd.read_csv(path + \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5159449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af48c868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c29c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_col = ['age','address','schooling','stud_hr','employed','h_disab',\n",
    "           'ment_cond','social_hr','fit_hr','wind','dry_mouth',\n",
    "           'positive','breath_diff','initiate','tremb','worry','look_fwd',\n",
    "           'down','enthus','life_mean','scared','outcome']\n",
    "df.columns= new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c1babc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Low signs of despression ', 'very Low/low Signs ',\n",
       "       'No signs of depression', 'Medium/High ',\n",
       "       'High signs of depression '], dtype=object)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.outcome.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add82bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "38a83423",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.copy()\n",
    "y = X.pop(\"outcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7577af02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 243 entries, 0 to 288\n",
      "Data columns (total 21 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   age          243 non-null    int64 \n",
      " 1   address      243 non-null    object\n",
      " 2   schooling    243 non-null    object\n",
      " 3   stud_hr      243 non-null    int64 \n",
      " 4   employed     243 non-null    object\n",
      " 5   h_disab      243 non-null    object\n",
      " 6   ment_cond    243 non-null    object\n",
      " 7   social_hr    243 non-null    object\n",
      " 8   fit_hr       243 non-null    object\n",
      " 9   wind         243 non-null    int64 \n",
      " 10  dry_mouth    243 non-null    int64 \n",
      " 11  positive     243 non-null    int64 \n",
      " 12  breath_diff  243 non-null    int64 \n",
      " 13  initiate     243 non-null    int64 \n",
      " 14  tremb        243 non-null    int64 \n",
      " 15  worry        243 non-null    int64 \n",
      " 16  look_fwd     243 non-null    int64 \n",
      " 17  down         243 non-null    int64 \n",
      " 18  enthus       243 non-null    int64 \n",
      " 19  life_mean    243 non-null    int64 \n",
      " 20  scared       243 non-null    int64 \n",
      "dtypes: int64(14), object(7)\n",
      "memory usage: 41.8+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd86e01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfd5d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "22b2316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.outcome.replace('high signs of depression ', 'High signs of depression ', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "96ccc713",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal=X.select_dtypes([\"int64\", \"float64\"]).nunique()==4\n",
    "for i in ordinal.index[ordinal]:\n",
    "    X[i]= X[i].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec61488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f678a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_mask=X.select_dtypes(\"object\").nunique()==2\n",
    "binary_columns=binary_mask.index[binary_mask]\n",
    "nominal_columns=binary_mask.index[~binary_mask]\n",
    "numerical_mask=X.select_dtypes([\"int64\", \"float64\"]).nunique()>4\n",
    "numerical_columns=numerical_mask.index[numerical_mask]\n",
    "category_rank_columns=X.select_dtypes(\"category\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345f86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b17dcd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = [\n",
    "    (\"ordinal columns\", OrdinalEncoder(), category_rank_columns),\n",
    "    (\"nominal columns\", OneHotEncoder(), nominal_columns),\n",
    "    (\"numerical columns\", StandardScaler(), numerical_columns)   \n",
    "]\n",
    "\n",
    "column_transformer = ColumnTransformer(transformer, remainder=\"passthrough\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dc9a67dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>-0.730930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>-0.730930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>0.063417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.310575</td>\n",
       "      <td>-1.048668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.769300</td>\n",
       "      <td>-1.366407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>-0.730930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283675</td>\n",
       "      <td>0.063417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.607700</td>\n",
       "      <td>0.063417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.580800</td>\n",
       "      <td>1.254937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877925</td>\n",
       "      <td>-0.730930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>243 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0    1    2    3    4    5    6    7    8    9   ...   22   23   24  \\\n",
       "0    1.0  2.0  1.0  0.0  2.0  1.0  1.0  2.0  1.0  2.0  ...  0.0  1.0  0.0   \n",
       "1    1.0  1.0  1.0  0.0  0.0  0.0  2.0  2.0  2.0  1.0  ...  0.0  0.0  1.0   \n",
       "2    3.0  2.0  2.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  1.0   \n",
       "3    1.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  ...  0.0  0.0  1.0   \n",
       "4    0.0  0.0  2.0  0.0  2.0  0.0  2.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "238  1.0  2.0  2.0  2.0  2.0  2.0  1.0  2.0  3.0  2.0  ...  0.0  0.0  1.0   \n",
       "239  1.0  1.0  1.0  0.0  1.0  0.0  2.0  0.0  1.0  1.0  ...  0.0  0.0  0.0   \n",
       "240  2.0  2.0  3.0  0.0  2.0  2.0  2.0  1.0  2.0  2.0  ...  1.0  0.0  1.0   \n",
       "241  1.0  0.0  2.0  1.0  2.0  1.0  3.0  1.0  1.0  1.0  ...  1.0  0.0  0.0   \n",
       "242  3.0  2.0  2.0  2.0  1.0  1.0  1.0  2.0  2.0  2.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "      25   26   27   28   29        30        31  \n",
       "0    0.0  0.0  0.0  1.0  0.0  0.580800 -0.730930  \n",
       "1    0.0  1.0  0.0  0.0  0.0  0.283675 -0.730930  \n",
       "2    0.0  0.0  0.0  1.0  0.0  0.283675  0.063417  \n",
       "3    0.0  1.0  0.0  0.0  0.0 -0.310575 -1.048668  \n",
       "4    1.0  0.0  1.0  0.0  0.0  1.769300 -1.366407  \n",
       "..   ...  ...  ...  ...  ...       ...       ...  \n",
       "238  0.0  1.0  0.0  0.0  0.0  0.283675 -0.730930  \n",
       "239  1.0  0.0  0.0  1.0  0.0  0.283675  0.063417  \n",
       "240  0.0  1.0  0.0  0.0  0.0 -0.607700  0.063417  \n",
       "241  1.0  0.0  0.0  1.0  0.0  0.580800  1.254937  \n",
       "242  1.0  0.0  0.0  0.0  1.0  0.877925 -0.730930  \n",
       "\n",
       "[243 rows x 32 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing the pipeline\n",
    "pd.DataFrame(column_transformer.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1590ed38",
   "metadata": {},
   "outputs": [],
   "source": [
    "classfier_names = [\"Decision Tree\", \"Randome Forrest\", \"Logistic CV\"]\n",
    "classfiers = [DecisionTreeClassifier(), RandomForestClassifier(), LogisticRegressionCV(max_iter=2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc155852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8ac7e78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators = [ Pipeline([  (\"transformer\", column_transformer), (classfier_name, classfier)])\n",
    "\n",
    "             for classfier_name, classfier in zip(classfier_names, classfiers)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0510fed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ff7ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3b22e73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "scoring = {\"acc\": 'accuracy'}\n",
    "training_scores = []\n",
    "cv_scores = {classfier:[] for classfier in classfier_names }\n",
    "for classfier_name, estimator in zip(classfier_names, estimators):\n",
    "    cv_score=cross_validate(estimator=estimator, X=X, y=y, cv =5, scoring=scoring, return_train_score=True,\n",
    "    return_estimator=True)\n",
    "    \n",
    "    training_score=np.mean(cv_score[\"train_acc\"]) * 100    \n",
    "    training_scores.append(training_score)\n",
    "    cv_scores[classfier_name].append(cv_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4a6a81f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Decision Tree': [{'fit_time': array([0.02605939, 0.0231688 , 0.0083046 , 0.01682329, 0.01667953]),\n",
       "   'score_time': array([0.01064086, 0.00801444, 0.02489948, 0.00588775, 0.01767588]),\n",
       "   'estimator': [Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Decision Tree', DecisionTreeClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Decision Tree', DecisionTreeClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Decision Tree', DecisionTreeClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Decision Tree', DecisionTreeClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Decision Tree', DecisionTreeClassifier())])],\n",
       "   'test_acc': array([0.67346939, 0.73469388, 0.79591837, 0.72916667, 0.64583333]),\n",
       "   'train_acc': array([0.97938144, 0.96391753, 0.95360825, 0.95384615, 0.99487179])}],\n",
       " 'Randome Forrest': [{'fit_time': array([0.23426056, 0.21900392, 0.38341165, 0.25764108, 0.23252201]),\n",
       "   'score_time': array([0.03336596, 0.02994227, 0.03904366, 0.02521729, 0.03324962]),\n",
       "   'estimator': [Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Randome Forrest', RandomForestClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Randome Forrest', RandomForestClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Randome Forrest', RandomForestClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Randome Forrest', RandomForestClassifier())]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Randome Forrest', RandomForestClassifier())])],\n",
       "   'test_acc': array([0.73469388, 0.71428571, 0.85714286, 0.8125    , 0.625     ]),\n",
       "   'train_acc': array([0.97938144, 0.96391753, 0.95360825, 0.95384615, 0.99487179])}],\n",
       " 'Logistic CV': [{'fit_time': array([12.28220916, 12.73359275, 13.42838907, 12.36527395, 10.19447351]),\n",
       "   'score_time': array([0.01420212, 0.01481247, 0.00197911, 0.01620221, 0.0139091 ]),\n",
       "   'estimator': [Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Logistic CV', LogisticRegressionCV(max_iter=2000))]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Logistic CV', LogisticRegressionCV(max_iter=2000))]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Logistic CV', LogisticRegressionCV(max_iter=2000))]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Logistic CV', LogisticRegressionCV(max_iter=2000))]),\n",
       "    Pipeline(steps=[('transformer',\n",
       "                     ColumnTransformer(remainder='passthrough',\n",
       "                                       transformers=[('ordinal columns',\n",
       "                                                      OrdinalEncoder(),\n",
       "                                                      Index(['wind', 'dry_mouth', 'positive', 'breath_diff', 'initiate', 'tremb',\n",
       "           'worry', 'look_fwd', 'down', 'enthus', 'life_mean', 'scared'],\n",
       "          dtype='object')),\n",
       "                                                     ('nominal columns',\n",
       "                                                      OneHotEncoder(),\n",
       "                                                      Index(['address', 'schooling', 'employed', 'h_disab', 'ment_cond', 'social_hr',\n",
       "           'fit_hr'],\n",
       "          dtype='object')),\n",
       "                                                     ('numerical columns',\n",
       "                                                      StandardScaler(),\n",
       "                                                      Index(['age', 'stud_hr'], dtype='object'))])),\n",
       "                    ('Logistic CV', LogisticRegressionCV(max_iter=2000))])],\n",
       "   'test_acc': array([0.59183673, 0.67346939, 0.7755102 , 0.8125    , 0.60416667]),\n",
       "   'train_acc': array([0.72680412, 0.7371134 , 0.77319588, 0.76923077, 0.87179487])}]}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b64d45de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[96.91250330425589, 96.91250330425589, 77.56278086174994]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fc9ac9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "best_estimator\n",
    "for i, y in cv_scores.items():\n",
    "    best_test_score=np.argmax(y[0][\"test_acc\"])\n",
    "    best_estimator=np.argmax(y[0][\"estimator\"][best_test_score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7996e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7386cb88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabbfc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1c6382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e6318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fce0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1752ea1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b486d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84125c75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5c2821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737635f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6483a2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ab78e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e218c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922cb155",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb48470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee048c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2922ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013dfdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f43f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6876ee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2211840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ffc7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd26ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eccbf89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d462b82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489bb7da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958a7c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aae777d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c18886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665849a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
